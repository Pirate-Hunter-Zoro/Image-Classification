{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will read in data as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Population Size: 600\n",
      "Test Population Size: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 28, 28)\n",
      "Class: 6\n"
     ]
    }
   ],
   "source": [
    "# The following line was necessary to import the MNIST images.\n",
    "# Source: https://stackoverflow.com/questions/78668638/unable-to-load-mnist-data-set-due-to-ssl-error-in-keras-load-data-function\n",
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries to include data and reading it in as images and arrays\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Load MNIST dataset as images\n",
    "train_dataset_as_images = dsets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "test_dataset_as_images = dsets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "# Load the MNIST dataset as arrays\n",
    "train_dataset_as_arrays = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataset_as_arrays = [(t.numpy(), c) for t,c in train_dataset_as_arrays]\n",
    "test_dataset_as_arrays = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataset_as_arrays = [(t.numpy(), c) for t,c in test_dataset_as_arrays]\n",
    "\n",
    "# The population is way too big for this notebook to handle in a reasonable amount of time - we'll scale down\n",
    "train_indices = range(len(train_dataset_as_arrays))\n",
    "train_sample_indices = random.sample(population=train_indices, k=int(0.01*len(train_indices)))\n",
    "test_indicies = range(len(test_dataset_as_arrays))\n",
    "test_sample_indices = random.sample(population=test_indicies, k=int(0.01*len(test_indicies)))\n",
    "train_dataset_as_arrays = [train_dataset_as_arrays[i] for i in train_sample_indices]\n",
    "test_dataset_as_arrays = [test_dataset_as_arrays[i] for i in test_sample_indices]\n",
    "print(f\"Train Population Size: {len(train_dataset_as_arrays)}\")\n",
    "print(f\"Test Population Size: {len(test_dataset_as_arrays)}\")\n",
    "\n",
    "# Display the first sample image\n",
    "first_image, first_label = train_dataset_as_images[0]\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.title(f\"Label: {first_label}\")\n",
    "plt.show()\n",
    "\n",
    "# Display said image as an array\n",
    "first_image_array, first_label = train_dataset_as_arrays[0]\n",
    "print(type(first_image_array))\n",
    "print(first_image_array.shape)\n",
    "print(f\"Class: {first_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to create a neural network - at heart a list of matrices. We will then use perceptrons - each perceptron will know to reference a certain row of a certain matrix - for book keeping when updating the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ANN:\n",
    "    \n",
    "    def __init__(self, input_dim: int, layer_dims: list[int], initialization_constant: int=1, lr: float=0.01):\n",
    "        \"\"\"Create an artificial neural network which is defined by the number of layers and the number of neurons per layer\n",
    "\n",
    "        Args:\n",
    "            layer_dims (list[int]): number of neurons per each respective layer\n",
    "            num_classifications (int): number of different classes each input could belong to (i.e. for handwritten digits, this will be 10 --> representing 0-9)\n",
    "            initialization_constant (int, optional): upper bound for maximum value of randomly initialized weights. Defaults to 1.\n",
    "            lr (float, optional): scalar used when performing gradient descent. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.__weight_matrices = [-initialization_constant + 2*initialization_constant*np.random.randn(input_dim, layer_dims[0])] + [-initialization_constant + 2*initialization_constant*np.random.randn(prev_dim, next_dim) for prev_dim, next_dim in zip(layer_dims, layer_dims[1:])]\n",
    "        \n",
    "        self.__derivatives = [np.zeros([prev_dim, next_dim]) for prev_dim, next_dim in zip(layer_dims, layer_dims[1:])]\n",
    "        # dL/dW_0, dL/dz_0, dL/da_0, dL/dW_1, dL/dz_1, dL/da_1, ..., etc.\n",
    "        self.__learning_rate = lr\n",
    "    \n",
    "    def __merge_network_elements(self, weights: list[np.ndarray], linear_outputs: list[np.ndarray], activation_outputs: list[np.ndarray]):\n",
    "        \"\"\"Helper method to merge the network elements - including weight matrices, linear output vectors, and activation output vectors into one list\n",
    "\n",
    "        Args:\n",
    "            weights (list[np.ndarray]): weight matrices\n",
    "            linear_outputs (list[np.ndarray]): linear output vectors\n",
    "            activation_outputs (list[np.ndarray]): activation output vectors\n",
    "        \"\"\"\n",
    "        # Merge into a list of [W0, z0, a0, W1, z1, a1, W2, z2, a2, etc...]\n",
    "        i = 0\n",
    "        network_components = [weights, linear_outputs, activation_outputs]\n",
    "        self.__network_elements = []\n",
    "        self.__weight_matrices = []\n",
    "        while any(network_components):\n",
    "            if network_components[i]:\n",
    "                next = network_components[i].pop(0)\n",
    "                if i == 0:\n",
    "                    self.__weight_matrices.append(next)\n",
    "                self.__network_elements.append(next)\n",
    "            i = (i + 1) % 3\n",
    "    \n",
    "    def __sigmoid(x: float) -> float:\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    __vector_sigmoid_func = np.frompyfunc(__sigmoid, 1, 1)\n",
    "    \n",
    "    def __sigmoid(vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Helper function to apply the sigmoid activation function to a numpy array\n",
    "\n",
    "        Args:\n",
    "            vector (np.ndarray): vector to apply sigmoid on (element-wise)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: resulting vector from sigmoid function\n",
    "        \"\"\"\n",
    "        return ANN.__vector_sigmoid_func(vector)\n",
    "\n",
    "    def __forward(self, input: np.ndarray, train: bool=True) -> np.ndarray:\n",
    "        \"\"\"Forward propagation for this neural network\n",
    "\n",
    "        Args:\n",
    "            input (np.ndarray): input vector to predict classification for\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: output vector to represent the prediction for the input vector\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        if train:\n",
    "            weights = []\n",
    "            linear_outputs = []\n",
    "            activation_outputs = []\n",
    "            for W in self.__weight_matrices:\n",
    "                weights.append(W)\n",
    "                x = np.matmul(x, W)\n",
    "                linear_outputs.append(x)\n",
    "                x = ANN.__sigmoid(x)\n",
    "                activation_outputs.append(x)\n",
    "            self.__merge_network_elements(weights=weights, linear_outputs=linear_outputs, activation_outputs=activation_outputs)\n",
    "        else:\n",
    "            for W in self.__weight_matrices:\n",
    "                weights.append(W)\n",
    "                x = np.matmul(x, W)\n",
    "                x = ANN.__sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def __calculate_gradients(self, last_input: np.ndarray, last_output: np.ndarray, target_output: np.ndarray):\n",
    "        \"\"\"Given the numerical loss received from a training observation, calculate the gradient of the loss and update each weight accordingly\n",
    "\n",
    "        Args:\n",
    "            last_output (np.ndarray): predicted output for the last input vector\n",
    "            target_output (np.ndarray): expected output for the last input vector (something like [0, 0, 0, 0, 1, 1, 1])\n",
    "        \"\"\"\n",
    "        # We know the immediate gradient for the last layer\n",
    "        self.__derivatives[-1] = 2 * (last_output - target_output) # vector\n",
    "        for i in range(len(self.__derivatives)-1, 0, -1):\n",
    "            if i % 3 == 0:\n",
    "                # weight matrix derivative\n",
    "                activation_output_layer = self.__network_elements[i-1] if i > 0 else last_input\n",
    "                # dL/dW_i_k_j = dL/dz_i_k * dz_i_k/dW_i_k_j = dL/dz_i_k * a_{i-1}_j\n",
    "                # dL/dW_i_k = dL/dz_i_k(a_{i-1})\n",
    "                # dL/dW_i = (dL/dz_i) X (a_{i-1})\n",
    "                dz_i_transpose = self.__derivatives[i+1].T\n",
    "                self.__derivatives[i] = np.matmul(dz_i_transpose, activation_output_layer)\n",
    "            elif i % 3 == 1:\n",
    "                # linear output layer derivative\n",
    "                activation_output_layer = self.__network_elements[i+1]\n",
    "                # dL/dz_i = dL/da_i * da_i/dz_i = dL/da_i * a_i * (1-a_i) since the derivative of sigmoid at x is sigmoid(x)(1-sigmoid(x))\n",
    "                self.__derivatives[i] = self.__derivatives[i+1] * activation_output_layer * (np.ones(shape=activation_output_layer.shape) - activation_output_layer)\n",
    "            else:\n",
    "                # activation layer derivative\n",
    "                next_weight_matrix = self.__network_elements[i+1]\n",
    "                # dL/da_i = (dL/dz_{i+1}) X (dz_{i+1}/da_i) = (dL/dz_{i+1}) X (W_{i+1}.T)\n",
    "                dz_next = self.__derivatives[i+2] \n",
    "                self.__derivatives[i] = np.matmul(dz_next, next_weight_matrix.T)\n",
    "        \n",
    "    def __update_weights(self):\n",
    "        \"\"\"Helper method to update the weights of our neural network\n",
    "        \"\"\"\n",
    "        for i in range(0, len(self.__derivatives)-2, 3):\n",
    "            self.__weight_matrices[i // 3] -= self.__learning_rate * self.__derivatives[i]\n",
    "            \n",
    "    def __perform_SGD(self, train_set: list[np.ndarray], expected_outputs: list[np.ndarray]) -> tuple[float,float]:\n",
    "        \"\"\"Given a batch of observations, calculate the average gradient and update all weights accordingly, returning the loss and accuracy of the network on this batch\n",
    "\n",
    "       Args:\n",
    "            train_set (list[np.ndarray]): input feature vectors\n",
    "            expected_outputs (list[np.ndarray]): expected outputs for each input vector\n",
    "\n",
    "        Returns:\n",
    "            tuple[float,float]: loss and accuracy\n",
    "        \"\"\"\n",
    "        average_loss = 0\n",
    "        average_accuracy = 0\n",
    "        for input, expected_output in zip(train_set, expected_outputs):\n",
    "            output = self.__forward(input=input)\n",
    "            self.__calculate_gradients(last_input=input, last_output=output, target_output=expected_output)\n",
    "            self.__update_weights()\n",
    "            average_loss += (1 / len(train_set)) * np.sum(np.square(output - expected_output))\n",
    "            predicted_class = np.argmax(output)\n",
    "            actual_class = np.argmax(expected_output)\n",
    "            if predicted_class == actual_class:\n",
    "                average_accuracy += 1 / len(train_set)\n",
    "        return (average_loss, average_accuracy)\n",
    "    \n",
    "    def __get_test_accuracy(self, test_set: list[np.ndarray], expected_outputs: list[np.ndarray]) -> float:\n",
    "        \"\"\"Calculate the testing accuracy for the list of observations which have not been used to train this network\n",
    "\n",
    "        Args:\n",
    "            test_set (list[np.ndarray]): set of observations which have not been used to train the network\n",
    "            expected_outputs (list[np.ndarray]): expected output results for each test observation\n",
    "\n",
    "        Returns:\n",
    "            float: accuracy of this network on the testing population\n",
    "        \"\"\"\n",
    "        average_accuracy = 0\n",
    "        for input, expected_output in zip(test_set, expected_outputs):\n",
    "            output = self.__forward(input=input, train=False)\n",
    "            predicted_class = np.argmax(output)\n",
    "            actual_class = np.argmax(expected_output)\n",
    "            if predicted_class == actual_class:\n",
    "                average_accuracy += 1 / len(test_set)\n",
    "        return average_accuracy\n",
    "    \n",
    "    def train_and_test(self, train_set: list[np.ndarray], expected_train_results: list[np.ndarray], test_set: list[np.ndarray], expected_test_results: list[np.ndarray], num_epochs: int=100):\n",
    "        \"\"\"Train the  network on the training observations, and test their results on the test populations\n",
    "\n",
    "        Args:\n",
    "            train_set (list[np.ndarray]): training population\n",
    "            expected_train_results (list[np.ndarray]): training population expected results\n",
    "            test_set (list[np.ndarray]): testing population\n",
    "            expected_test_results (list[np.ndarray]): testing population results\n",
    "            num_empochs (int, optional): number of times the network trains through the entire epoch. Defaults to 100.\n",
    "        \"\"\"\n",
    "        training_loss_by_epoch = []\n",
    "        training_accuracy_by_epoch = []\n",
    "        testing_accuracy_by_epoch = []\n",
    "        for i in range(num_epochs):\n",
    "            train_loss, train_accuracy = self.__perform_SGD(train_set=train_set, expected_outputs=expected_train_results)\n",
    "            training_loss_by_epoch.append(train_loss)\n",
    "            training_accuracy_by_epoch.append(train_accuracy)\n",
    "            test_accuracy = self.__get_test_accuracy(test_set=test_set, expected_outputs=expected_test_results)\n",
    "            testing_accuracy_by_epoch.append(test_accuracy)\n",
    "            if i % (num_epochs // 10) == 0:\n",
    "                print(f\"{i/num_epochs * 100}% through training...\")\n",
    "        \n",
    "        # Plot training loss\n",
    "        plt.plot(range(num_epochs), training_loss_by_epoch, 'rx')\n",
    "        plt.title(\"Training Loss by Epoch\")\n",
    "        plt.xlabel(\"Epoch Number\")\n",
    "        plt.ylabel(\"Training Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot training accuracy\n",
    "        plt.plot(range(num_epochs), training_accuracy_by_epoch, 'go')\n",
    "        plt.title(\"Training Accuracy by Epoch\")\n",
    "        plt.xlabel(\"Epoch Number\")\n",
    "        plt.ylabel(\"Training Accuracy\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot testing accuracy\n",
    "        plt.plot(range(num_epochs), testing_accuracy_by_epoch, 'go')\n",
    "        plt.title(\"Testing Accuracy by Epoch\")\n",
    "        plt.xlabel(\"Epoch Number\")\n",
    "        plt.ylabel(\"Testing Accuracy\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us create an ANN and train it on the MNIST images...\n",
    "neural_network = ANN(input_dim=28*28, layer_dims=[28*14, 28*7, 28*3, 28, 14, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training input vectors\n",
    "train_vectors = [array_class_pair[0] for array_class_pair in train_dataset_as_arrays]\n",
    "# Create expected output vector results\n",
    "def make_output_array(classification: int) -> np.ndarray:\n",
    "    x = np.zeros(shape=(1,10))\n",
    "    x[0][classification] = 1.0\n",
    "    return x\n",
    "expected_train_outputs = [make_output_array(classification=array_class_pair[1]) for array_class_pair in train_dataset_as_arrays]\n",
    "\n",
    "# Now do the same for the test population\n",
    "train_vectors = [array_class_pair[0] for array_class_pair in test_dataset_as_arrays]\n",
    "expected_test_outputs = [make_output_array(classification=array_class_pair[1]) for array_class_pair in test_dataset_as_arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mneural_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset_as_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_train_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_train_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset_as_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_test_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_test_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 179\u001b[0m, in \u001b[0;36mANN.train_and_test\u001b[0;34m(self, train_set, expected_train_results, test_set, expected_test_results, num_epochs)\u001b[0m\n\u001b[1;32m    177\u001b[0m testing_accuracy_by_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 179\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__perform_SGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_train_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     training_loss_by_epoch\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    181\u001b[0m     training_accuracy_by_epoch\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n",
      "Cell \u001b[0;32mIn[2], line 136\u001b[0m, in \u001b[0;36mANN.__perform_SGD\u001b[0;34m(self, train_set, expected_outputs)\u001b[0m\n\u001b[1;32m    134\u001b[0m average_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, expected_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_set, expected_outputs):\n\u001b[0;32m--> 136\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__calculate_gradients(last_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, last_output\u001b[38;5;241m=\u001b[39moutput, target_output\u001b[38;5;241m=\u001b[39mexpected_output)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_weights()\n",
      "Cell \u001b[0;32mIn[2], line 75\u001b[0m, in \u001b[0;36mANN.__forward\u001b[0;34m(self, input, train)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m W \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__weight_matrices:\n\u001b[1;32m     74\u001b[0m     weights\u001b[38;5;241m.\u001b[39mappend(W)\n\u001b[0;32m---> 75\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     linear_outputs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m ANN\u001b[38;5;241m.\u001b[39m__sigmoid(x)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "neural_network.train_and_test(train_set=train_dataset_as_arrays, expected_train_results=expected_train_outputs, test_set=test_dataset_as_arrays, expected_test_results=expected_test_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
