{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we define a Decision Tree.\n",
    "#### A Decision Tree is initialized with some training data and testing data, as well as a list of discrete-valued attributes that each observation will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first define what it means to be a discrete-valued attribute - each observation will have a list of these items\n",
    "from abc import ABC\n",
    "from typing_extensions import Self\n",
    "\n",
    "\n",
    "class Attribute:\n",
    "    \"\"\"An attribute is defined by its set of possible values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, values: set[int]):\n",
    "        \"\"\"Initialize an Attribute object\n",
    "\n",
    "        Args:\n",
    "            values (set[int]): set of all possible assignments for any instance of this attribute\n",
    "        \"\"\"\n",
    "        self.__values = values\n",
    "        \n",
    "    def contains_option(self, v: int) -> bool:\n",
    "        \"\"\"Helper method to determine if this attribute has the input value as an option (e.g. - a binary attribute will not have 2 or a greater number as a possible value)\n",
    "\n",
    "        Args:\n",
    "            v (int): value in question \n",
    "\n",
    "        Returns:\n",
    "            bool: whether the value is a possible option for this Attribute\n",
    "        \"\"\"\n",
    "        return v in self.__values\n",
    "    \n",
    "        \n",
    "class AttributeInstance:\n",
    "    \"\"\"When an observation is created, it will be a given a list of Attributes, and a list of AttributeInstances - a value for said attribute\n",
    "    \"\"\"       \n",
    "    \n",
    "    def __init__(self, attribute: Attribute, value: int):\n",
    "        \"\"\"Create an Attribute Instance object, which needs to know what attribute it is an instance of, and its assignment\n",
    "\n",
    "        Args:\n",
    "            attribute (Attribute): The underlying attribute with its list of possible values\n",
    "            value (int): Value assigned to this particular attribute instance\n",
    "        \"\"\"\n",
    "        assert attribute.contains_option(value)\n",
    "        self.__attribute = attribute\n",
    "        self.__value = value\n",
    "        \n",
    "    def get_value(self, attribute: Attribute) -> int:\n",
    "        \"\"\"If the attribute matches this attribute instance's underlying attribute, return this instance's associated value\n",
    "\n",
    "        Args:\n",
    "            v (int): Value to compare to this AttributeInstance's value\n",
    "        \"\"\"\n",
    "        if self.__attribute == attribute:\n",
    "            return self.__value\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "class Observation(ABC):\n",
    "    \"\"\"An Observation will have a list of attributes and an equally long list of attribute instances\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classification: int, attributes: list[Attribute], values: list[int]):\n",
    "        \"\"\"Create an observation with its list of attributes and its list of corresponding values for each attribute\n",
    "\n",
    "        Args:\n",
    "            classification (int): Classification for this observation\n",
    "            attributes (list[Attribute]): List of attributes which apply to this observation\n",
    "            values (list[AttributeInstance]): List of this observation's values for each of the attributes\n",
    "        \"\"\"\n",
    "        self.__classification = classification\n",
    "        self.attributes = attributes\n",
    "        self.__values = [AttributeInstance(attribute=attr, value=v) for attr,v in zip(attributes, values)]\n",
    "        \n",
    "    def get_class(self) -> int:\n",
    "        \"\"\"Return the class of this observation\n",
    "\n",
    "        Returns:\n",
    "            int: this observation's class\n",
    "        \"\"\"\n",
    "        return self.__classification\n",
    "        \n",
    "    def get_value(self, attribute: Attribute) -> tuple[int, AttributeInstance]:\n",
    "        \"\"\"For the given attribute, return its value for this observation\n",
    "\n",
    "        Args:\n",
    "            attribute (Attribute): attribute whose value we want for this observation\n",
    "\n",
    "        Returns:\n",
    "            int: value for \n",
    "        \"\"\"\n",
    "        for attr_inst in self.__values:\n",
    "            if attr_inst.get_value(attribute=attribute) != None:\n",
    "                return (attr_inst.get_value(), attr_inst)\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def partition(observations: list[Self], attribute: Attribute) -> list[list[Self]]:\n",
    "        \"\"\"Class method that - given a list of observations, partition them by their value of the given attribute\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        seen = {}\n",
    "        for observation in observations:\n",
    "            v = observation.get_value(attribute)\n",
    "            if v in seen.keys():\n",
    "                # Grab the list of observations associated with this value, and append this observation to it\n",
    "                result[seen[v]][0].append(observation)\n",
    "            else:\n",
    "                seen[v] = len(result)\n",
    "                result.append([observation])\n",
    "        \n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a decision tree which - given a list of training Observations - produces a decision tree that will predict classes of unseen Observations\n",
    "class DecisionTree:\n",
    "    \"\"\"This will essentially be a wrapper for a tree node class - the node will do all the work\n",
    "    \"\"\"\n",
    "    \n",
    "    class DecisionTreeNode:\n",
    "        \"\"\"An inner class to help make decisions for observations based on attributes\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, observations: list[Observation], attributes: list[Attribute], use_gini_index: bool=False):\n",
    "            \"\"\"Initialize a DecisionTreeNode with a given list of observations and attributes the data needs to be split on\n",
    "\n",
    "            Args:\n",
    "                observations (list[Observation]): observations, each of which have values for each attribute in attributes\n",
    "                attributes (list[Attribute]): list of attributes the data still needs to be split on\n",
    "            \"\"\"\n",
    "            self.__gini_index = use_gini_index\n",
    "            self.__default_prediction = None\n",
    "            self.__children = {}\n",
    "            self.__attribute_to_split_on = None\n",
    "            # First check of all predictions are of the same class\n",
    "            classes = set([observation.get_class() for observation in observations])\n",
    "            if len(classes) == 1:\n",
    "                self.__default_prediction = observations[0].get_class()\n",
    "            else:\n",
    "                # See which attribute is the most valuable to split on\n",
    "                record = float('-inf')\n",
    "                for attribute in attributes:\n",
    "                    categories = Observation.partition(observations=observations, attribute=attribute)\n",
    "                    value = self.__calculate_value(partition=categories)\n",
    "                    if value > record:\n",
    "                        record = value\n",
    "                        self.__attribute_to_split_on = attribute\n",
    "                        \n",
    "                if self.__attribute_to_split_on:\n",
    "                    # We did have attributes to split on\n",
    "                    categories = Observation.partition(observations=observations, attribute=self.__attribute_to_split_on)\n",
    "                    new_attributes = [attribute for attribute in attributes if attribute != self.__attribute_to_split_on]\n",
    "                    for group_class in categories:\n",
    "                        # Each tuple has a list of observations and the value for the split-on attribute for each list\n",
    "                        self.__children[group_class[1]] = DecisionTree.DecisionTreeNode(observations=group_class[0], attributes=new_attributes, use_gini_index=self.__gini_index)\n",
    "                else:\n",
    "                    # Then we  have no attributes to branch on so go by majority classification\n",
    "                    counts = {}\n",
    "                    record = 0\n",
    "                    for observation in observations:\n",
    "                        classification = observation.get_class()\n",
    "                        if classification in counts.keys():\n",
    "                            counts[classification] += 1\n",
    "                        else:\n",
    "                            counts[classification] = 1\n",
    "                        if counts[classification] > record:\n",
    "                            record = counts[classification]\n",
    "                            self.__default_prediction = classification\n",
    "                    \n",
    "        def __calculate_value(self, partition: list[list[Observation]]) -> float:\n",
    "            \"\"\"Given a partition set of observations with respect to some attribute, return the value of said attribute\n",
    "\n",
    "            Args:\n",
    "                partition (list[tuple[list[Observation], AttributeInstance]]): Each tuple corresponds to a list of observations which share the same value for the partitioned attribute\n",
    "\n",
    "            Returns:\n",
    "                float: value associated with this partition of observations\n",
    "            \"\"\"\n",
    "            num_observations = sum([len(family) for family in partition])\n",
    "            classes = set([observation.get_class() for family in partition for observation in family])\n",
    "            if self.__gini_index:\n",
    "                pass\n",
    "            else:\n",
    "                result = 0\n",
    "                for family in partition:\n",
    "                    pass\n",
    "                        \n",
    "        \n",
    "        def predict(self, observation: Observation) -> int:\n",
    "            \"\"\"Predict the value of an unseen testing observation\n",
    "\n",
    "            Args:\n",
    "                observation (Observation): observation whose class we want to predict\n",
    "\n",
    "            Returns:\n",
    "                int: predicted classification for said observation\n",
    "            \"\"\"\n",
    "            if len(self.__children) == 0:\n",
    "                # This is a node where we went with a default prediction value because we have no attributes to split or all observations in training were of the same class\n",
    "                return self.__default_prediction\n",
    "            else:\n",
    "                return self.__children[observation.get_value(attribute=self.__attribute_to_split_on)].predict(observation=observation)\n",
    "                \n",
    "    \n",
    "    def __init__(self, observations: list[Observation]):\n",
    "        \"\"\"Create a decision tree based on the observations\n",
    "\n",
    "        Args:\n",
    "            observations (list[Observation]): list of observations that will define our tree\n",
    "        \"\"\"\n",
    "        self.__root = DecisionTree.DecisionTreeNode(observations = observations, attributes = observations[0].attributes)\n",
    "    \n",
    "    \n",
    "    def predict(self, observation: Observation) -> int:\n",
    "        \"\"\"Predict the classification of the given observation by having its underlying node do all the work\n",
    "\n",
    "        Args:\n",
    "            observation (Observation): observation whose class we want to predict\n",
    "\n",
    "        Returns:\n",
    "            int: prediction class of the inputted observation\n",
    "        \"\"\"\n",
    "        return self.__root.predict(observation=observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us the reading of observations from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class FileObservationsReader:\n",
    "    \"\"\"This class takes in data as a pandas data frame along with a block size and parameter for the k nearest neighbors, \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_name: str, delimiter: str, id_column_name: str, classification_column_name: str, k : int, n : int):\n",
    "        \"\"\"Initialize a FileObservationsReader, which creates a list of observations from the given file\n",
    "        Args:\n",
    "            file_name (str): The file from which to read in data\n",
    "            k (int): The number of closest other points used to guess if a certain point belongs to a given classification\n",
    "            n (int): The number of different blocks to partition the training points into\n",
    "        \"\"\"\n",
    "        # First let's read in our data\n",
    "        read_from = open(file_name, \"r\")\n",
    "        points_as_strings = read_from.readlines()\n",
    "\n",
    "        # Let's look at the first line, which reveals the point attributes which are numeric\n",
    "        header = points_as_strings[0][:-1] # Remember to omit the '\\n' character\n",
    "        attributes = header.split(delimiter)\n",
    "        class_idx = -1\n",
    "        id_idx = -1\n",
    "        attribute_name_indices = []\n",
    "        # In case keep track of all the values we see for each attribute, which will help us determine if the attribute is discrete or continuous\n",
    "        # Note that continuous attributes need to be discretized\n",
    "        attribute_values = {}\n",
    "        for i, attr in enumerate(attributes):\n",
    "            if attr == classification_column_name:\n",
    "                class_idx = i\n",
    "            elif attr == id_column_name:\n",
    "                id_idx = i\n",
    "            else:\n",
    "                attribute_name_indices.append(i)\n",
    "                attribute_values[i] = set()\n",
    "        points_as_strings = points_as_strings[1:]\n",
    "\n",
    "        read_from.close()\n",
    "        unique_labels = set()\n",
    "        self.__labels = []\n",
    "        self.__names = []\n",
    "        self.__points_as_dicts = []\n",
    "        for observation_idx, point_string in enumerate(points_as_strings):\n",
    "            # First omit the '\\n' character in each row of data corresponding to a point\n",
    "            point_attributes = point_string[:-1].split(delimiter)\n",
    "\n",
    "            # Record the unique id and class of this point\n",
    "            self.__labels.append(int(point_attributes[class_idx]))\n",
    "            unique_labels.add(int(point_attributes[class_idx]))\n",
    "            self.__names.append(point_attributes[id_idx])\n",
    "\n",
    "            # Turn this point into a dictionary\n",
    "            point_as_dict = {}\n",
    "\n",
    "            # Record all attribute elements of the points \n",
    "            for idx in attribute_name_indices:\n",
    "                attribute_values[idx].add(point_attributes[idx])\n",
    "                point_as_dict[idx] = point_attributes[idx]\n",
    "                \n",
    "            point_as_dict[id_column_name] = self.__names[observation_idx]\n",
    "            point_as_dict[classification_column_name] = self.__labels[observation_idx]\n",
    "            self.__points_as_dicts.append(point_as_dict)\n",
    " \n",
    "        # We need to look through each of the attribute value sets, and see which are continuous\n",
    "        # The following 3 dictionaries map attribute indices (as they appear in the table) to the set of possible values they take on\n",
    "        integer_discrete_attributes = {}\n",
    "        continuous_attributes = {}\n",
    "        named_discrete_attributes = {}\n",
    "        for idx, values in attribute_values.items():\n",
    "            for string in values:\n",
    "                if string.isdigit():\n",
    "                    # Discrete attribute\n",
    "                    if idx in integer_discrete_attributes.keys():\n",
    "                        integer_discrete_attributes[idx].add(int(string))\n",
    "                    else:\n",
    "                        integer_discrete_attributes[idx] = set([int(string)])\n",
    "                else:\n",
    "                    # See if it is a float - otherwise it is some discrete attribute\n",
    "                    try:\n",
    "                        if idx in continuous_attributes.keys():\n",
    "                            continuous_attributes[idx].append(float(string))\n",
    "                        else:\n",
    "                            continuous_attributes[idx] = [float(string)]\n",
    "                    except:\n",
    "                        # Not a float\n",
    "                        if idx in named_discrete_attributes.keys():\n",
    "                            named_discrete_attributes[idx].add(string)\n",
    "                        else:\n",
    "                            named_discrete_attributes[idx] = set([string])\n",
    "        \n",
    "        # Next we should normalize and then discretize the continuous attributes\n",
    "        def classify(x: float) -> int:\n",
    "            \"\"\"Depending on the value of a float - between 0 and 1 - give one of four discrete classifications\n",
    "\n",
    "            Args:\n",
    "                x (float): the float to classify\n",
    "\n",
    "            Returns:\n",
    "                int: the classification of said float\n",
    "            \"\"\"\n",
    "            if x < 0.25:\n",
    "                return 0\n",
    "            elif x < 0.5:\n",
    "                return 1\n",
    "            elif x < 0.75:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "        # Use that function\n",
    "        for attr_idx in continuous_attributes.keys():\n",
    "            array = np.array(continuous_attributes[attr_idx])\n",
    "            normalized = normalize(X=array, norm='l2', axis=1, copy=True).tolist()\n",
    "            continuous_attributes[attr_idx] = [classify(x) for x in normalized]\n",
    "            \n",
    "        # Now turn the named discrete attributes into integers\n",
    "        for attr_idx, possible_names in named_discrete_attributes.items():\n",
    "            # TODO\n",
    "            pass\n",
    "                    \n",
    "        # Now we can create our list of attributes, and all possible values for said attributes\n",
    "        \n",
    "\n",
    "        # Now we are ready to create a list of observations\n",
    "        self.__observations = []\n",
    "        # Each numerical attribute must be discretized\n",
    "        # TODO: next\n",
    "        # TODO: NON-discretized data?\n",
    "        \n",
    "        # For each data point, store its actual label keyed by its id\n",
    "        self.__values = {}\n",
    "        \n",
    "        # Store our model's TEST predictions of each point's label by that point's id in the TEST set\n",
    "        self.__test_predictions = {}\n",
    "        # Store our model's TRAIN predictions of each point's label by that point's id in the TRAIN set\n",
    "        self.__train_predictions = {}\n",
    "        \n",
    "        # This dictionary maps point id to that point's vector\n",
    "        point_vectors = {}\n",
    "        \n",
    "        # Record all of our points\n",
    "        for index, row in enumerate(self.__points_as_dicts):\n",
    "            self.__values[row[id_column_name]] = row[classification_column_name]\n",
    "            point_vectors[row[id_column_name]] = self.__normalized_data_array[index]\n",
    "            self.__test_predictions[row[id_column_name]] = None\n",
    "            self.__train_predictions[row[id_column_name]] = None\n",
    "            \n",
    "                    \n",
    "    def plot_predictions(self):\n",
    "        # If we do not have two-dimensional data, do not plot\n",
    "        if self.__raw_data_array.shape[1] != 2:\n",
    "            return\n",
    "\n",
    "        # Plot our data with its actual classifications\n",
    "        fig, ax = plt.subplots()\n",
    "        scatter = ax.scatter(self.__raw_data_array[:, 0], self.__raw_data_array[:, 1], c=self.__labels)\n",
    "        # Produce a legend with the unique colors from the scatter\n",
    "        # Source: https://matplotlib.org/stable/gallery/lines_bars_and_markers/scatter_with_legend.html#sphx-glr-gallery-lines-bars-and-markers-scatter-with-legend-py\n",
    "        legend = ax.legend(*scatter.legend_elements(),\n",
    "                            loc=\"lower left\", title=\"Classes\")\n",
    "        ax.add_artist(legend)\n",
    "        plt.title(\"Model Data\")\n",
    "        plt.xlabel(\"X Coordinate\")\n",
    "        plt.ylabel(\"Y Coordinate\")\n",
    "        plt.show()\n",
    "        fig.clear()\n",
    "        \n",
    "        # Now do the dame with predictions\n",
    "        fig, ax = plt.subplots()\n",
    "        scatter = ax.scatter(self.__raw_data_array[:, 0], self.__raw_data_array[:, 1], c=[self.__test_predictions[name] for name in self.__names])\n",
    "        # Produce a legend with the unique colors from the scatter\n",
    "        # Source: https://matplotlib.org/stable/gallery/lines_bars_and_markers/scatter_with_legend.html#sphx-glr-gallery-lines-bars-and-markers-scatter-with-legend-py\n",
    "        # legend = ax.legend(*scatter.legend_elements(),\n",
    "        #                     loc=\"lower left\", title=\"Classes\")\n",
    "        # ax.add_artist(legend)\n",
    "        plt.title(\"Model Test Prediction\")\n",
    "        plt.xlabel(\"X Coordinate\")\n",
    "        plt.ylabel(\"Y Coordinate\")\n",
    "        plt.show()\n",
    "        fig.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create an extension of the Observation class with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 28, 28)\n",
      "Class: 5\n"
     ]
    }
   ],
   "source": [
    "# The following line was necessary to import the MNIST images.\n",
    "# Source: https://stackoverflow.com/questions/78668638/unable-to-load-mnist-data-set-due-to-ssl-error-in-keras-load-data-function\n",
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries to include data and reading it in as images and arrays\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset as images\n",
    "train_dataset_as_images = dsets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "test_dataset_as_images = dsets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "# Load the MNIST dataset as arrays\n",
    "train_dataset_as_arrays = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataset_as_arrays = [(t.numpy(), c) for t,c in train_dataset_as_arrays]\n",
    "test_dataset_as_arrays = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataset_as_arrays = [(t.numpy(), c) for t,c in test_dataset_as_arrays]\n",
    "\n",
    "# Display the first sample image\n",
    "first_image, first_label = train_dataset_as_images[0]\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.title(f\"Label: {first_label}\")\n",
    "plt.show()\n",
    "\n",
    "# Display said image as an array\n",
    "first_image_array, first_label = train_dataset_as_arrays[0]\n",
    "print(type(first_image_array))\n",
    "print(first_image_array.shape)\n",
    "print(f\"Class: {first_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
